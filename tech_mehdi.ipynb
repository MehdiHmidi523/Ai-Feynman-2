{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%%\n"
    }
   },
   "source": [
    "Mehdi Hmidi, Deep learning Tech Blog, 1st April 2021.\n",
    "# <center> Ai Feynman: Symbolic Regression and Unknown Physics Formulae </center>\n",
    " <center> Intelligible Intelligence </center>\n",
    "\n",
    "![title](img/research.jpg)\n",
    "\n",
    "![title](img/index.jpg)\n",
    "\n",
    "Only a few decades ago, the ability of organisms to reproduce seemed to be a deep complex mystery and yet as soon as scientists understood the elements of how our gene strings replicate themselves, biologists wondered why it took so long to think of such a simple thing?\n",
    "\n",
    "During the 17th century, Newton discovered three simple laws that explain most of all the mechanical phenomena we see. A century or two later Maxwell, did the same thing for electricity with four simple laws that explained electricity and magnitism. Then Einstein came along and basically reduced those seven laws to really just one or two laws. And physics progressed wonderfully by finding these incredibly simple explanations.\n",
    "\n",
    "Today, scientific endeavours record lots of observational data. What usually happens is that scientists attempt to generalize such data into the simplest underlying formulations i.e. equations that “fit” the observations. These attempts are what mathematicians call symbolic regression. In essence, it boils down to building an equation for an output \"y\" that best fits a provided dataset.\n",
    "\n",
    "In other words, given a table of numbers, whose rows are of the form {x1,..., xn, y} where y = f(x1, ..., xn) the task is to search the space of symbolic expressions to find a model that best fits an unknown mystery function f.\n",
    "\n",
    "A simple explanation looks like this:\n",
    "\n",
    "![title](img/math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, when faced with difficult-to-determine functions, many approach symbolic regression by understanding how the smallest parts work and how combinations of them work and so forth to finalize a successful solution. But, sometimes, albeit most of the times, this process takes quite the duration to crystalize into an equation. Not to mention the mental effort and proneness to mistake, this resolution engenders.\n",
    "\n",
    "In computer terms, this is called NP-hard. It is easy to see why. Much like the number of passwords of length n, grow exponentially with n, the number of symbolic expressions of length n grows exponentially with n as well.\n",
    "\n",
    "So, even if we attempted to discover known formulas with a brute force approach it would take quite the long duration. By leveraging a reverse polish notation the researchers can see that the Planck BlackBody formula for instance could take longer than the age of our universe to be discovered by a brute force approach.\n",
    "\n",
    "\n",
    "![title](img/long.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1602, it took Johannes Kepler, 4 years and more than 40 failed attempts by hand to fit the Mars’ orbit to an ellipse and disrupt astronomy and the scientific understanding at the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today however, we are better equipped to tackle these problems.\n",
    "\n",
    "![title](img/explain.png)\n",
    "\n",
    "\n",
    "I like the idea of having more options rather than less. In fact, there are all sorts of ways mankind could end up locked in a particular way of looking at things but sometimes none of them could be very good. That's why we need tools to help in this process. With progress and understanding there must be something better than all the things humans have done so far.\n",
    "\n",
    "On this point, a research group led by Dr. Max Tegmark at MIT's Computer Science and Artificial Intelligence Laboratory, attempts to better automate and tackle this search problem. Their recent work \"AI Feynman\" and \"AI Poincaré\" drew much of the scientific community's attention when they improved upon the state of the art with a considerable margin.\n",
    "\n",
    "They argue that mathematicians are sometimes too quick to dismiss the possibility that we can solve something.\n",
    "Advanced Mathematics can be really difficult, and people sometimes overlook things. So, instead of waiting on outlier scientists or mathematicians to be born every couple of centuries, maybe one day with the help of computers and clever strategies we can discover equations and formulas that have not been pursued before, cut down the time to solve such problems and improve existing ones to better help humanity.\n",
    "\n",
    "In this article, I will explore the improvements they made in symbolic regression, their physics-inspired strategies and how they leveraged hidden simplicity in the data of interest. I will also investigate how they recursively broke hard problems into simpler ones with fewer variables by leveraging a variety of algorithmic methods. Their use of neural networks and lifelong learning will also be a topic for review.\n",
    "\n",
    "![title](img/men.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Intelligible intelligence strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more you can understand how your machine learning system actually works the more reasonable it is that you might trust it. Unlike explainability, which is a more ambitious goal that aims at creating systems capable of explaining their reasoning in human terms. Intelligible intelligence concerns itself with the ability to understand a solution at a deeper level to enable trust in its processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a physics background Dr. Tegmark thought to borrow four old ideas that have been successful in physics and early computer complexity theory and deploy a machine learning version of them. The strategies are the following:\n",
    "\n",
    "![title](img/four.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In physics, \"Occam's razor\" essentially boils down to the understanding that if you have a simple explanation that is just as accurate as a complicated one, you should prefer the simpler one. Spear headed by Dr. Ray Solomonoff and Dr. Andrey Kolmogorov, this idea was put into a firm mathematical footing with complexity theory. The only problem is that their definition of simple complexity is NP-hard to evaluate.\n",
    "\n",
    "To counteract this, the research team came up with a much simpler set of complexity criteria that are in a sense an easy-to-compute compromise, for their machine learning solutions which turns out to be very fast to evaluate.\n",
    "\n",
    "![title](img/compromise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They argued that:\n",
    "\n",
    "> If given an integer then the amount of bits of information one really needs to store is just how many digits-long it is in binary. Essentially, the log of the integer captures this number.\n",
    "\n",
    "> If it's a rational number then consider the complexity of the two integers, the complexity of the numerator plus the complexity of the denominator. Techniques are available to find suitable fractions.\n",
    "\n",
    "> If it's a real number, however, one may convert it to an integer by dividing by the precision floor of the computer's CPU and then taking the logarithm of the result and so forth.\n",
    "\n",
    "![title](img/crit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this point, on the y-axis of the first graph, the example is made with an interval of generic real numbers where the complexity grows as a logarithm of the values (red line). Change the precision floor and the red line could scale up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow through with their new complexity criteria, it became apparent that a new loss function is needed.\n",
    "A simple new measure of complexity of numbers that gives a much more robust method for fitting the data than the statistical chi-squared-test or minimizing the mean squared error (MSE) approach.\n",
    "\n",
    "![title](img/loss_func.png)\n",
    "\n",
    "\n",
    "you can immediately tell that if one has unwanted data points as in the left side, the mean squared error will put a lot of weight to those points that are quite far from the model and de facto compromise and pull away a little bit towards the outlier data points. \n",
    "\n",
    "However, the group's information theory approach has the opposite incentive. Their idea is to instruct the model to keep being more accurate on the data points it can already do accurately on and ignore the rest.\n",
    "\n",
    "Now, you might ask yourself <b>why do this?</b> this goes against the mainstream classic way of fitting data to a model in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there is no need for concern. In fact, this leads us to the second strategy at play here. One that is called \"Divide & Conquer\". In this spirit what they essentially attempt to do is to leverage an ensemble of models instead of creating one encompassing predictive model for the entire dataset.\n",
    "\n",
    "In concert, each model is incentivized to specialize and focus well on some parts or aspects of the data. And, grounded in a new complexity criterion what is left to do, is to converge the different loss functions to see how well the ensemble does.\n",
    "\n",
    "![title](img/harmonic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ensemble must strike a balance between the individual models, a harmonic mean can be considered a great evaluation metric for this purpose. \n",
    "\n",
    "In the end much like a human physicist or an undergraduate student, the machine learning solution put forward by the research group is put in different environements and instructed to use previously learned solutions, and ultimately unify and apply them to new ones. In other words, once coupled, these strategies deploy a lifelong learning strategy in the following way: \n",
    "\n",
    "![title](img/lifelong.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Problem formulation\n",
    "\n",
    "During the past decade, there has been some amazing work done in distilling free-form natural laws from experimental data. In fact, for about a decade, the state of the art of this field, has been dominated by one genetic algorithm. \n",
    "https://science.sciencemag.org/content/324/5923/81\n",
    "\n",
    "This program is called \"Eureqa\" and it was created by Hod lispson and Michael Schmidt to document analytical laws. To their credit, they achieved great results. However one must recognize that automating symbolic regression discovery in a process is no easy task. And that what they achieved does not seem to capture most of the equations known to scientists. It finds some but fails on a number of others.\n",
    "\n",
    "In response to that, MIT CSAIL's research team decided to tackle this problem in a different way. They initially thought it best to approach this task by breaking it down into manageable pieces. Before this can be achieved however, one must grasp the properties that concern the functions of practical interest.\n",
    "\n",
    "But wait a minute, <b>which functions does the field concern itself with?</b>\n",
    "\n",
    "In fact, this is the main criticism of symbolic regression research. It is the the lack of a benchmark. \n",
    "\n",
    "So before anything, Dr. Tegmark and his Ph.D. students brought upon the creation of a new benchmark in numerical experiments to enable quantitative benchmarking. What they initially did before expanding this benchmark, was turn the most complicated equations from the Feynman lectures on physics into datatables that are freely downloadable.\n",
    "\n",
    "https://space.mit.edu/home/tegmark/aifeynman.html\n",
    "\n",
    "\n",
    "![title](img/bench.png)\n",
    "\n",
    "Once this was done, the next step in the process was to try and exploit the properties of these functions. Building upon previous work in learning functions, they noticed that even if some random generic functions are NP-hard to find, they will have some special properties https://arxiv.org/pdf/1603.00988.pdf\n",
    "\n",
    "Previous work done by T. Poggio reinforces this claim. The study emphasized that most formulas we care about are compositional. So in a way, even if a formula is composed of nine variables, one can usually rewrite it as a combination of functions with fewer variables.\n",
    "\n",
    "On this point, one of things they observed was that unlike generic functions some of the functions they cared about have a modular computational graph.\n",
    "\n",
    "![title](img/decompose.png)\n",
    "\n",
    "Here we can see an example of a function of three variables that can be decomposed as a combination of two functions with two variables.\n",
    "\n",
    "By applying recursive tests on the mystery function, the research team proves that they can discover any graph modularity found in the data. In turn, this allows them to leverage this graph modularity for their purposes. In a way, they can work out ever simpler modules, until the mystery functions become simple enough that they figure out what they are. Essentially this is the \"Divide and Conquer\" approach.\n",
    "\n",
    "![title](img/divide.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein, functions often display <b>symmetries</b> and <b>separability</b>. Maybe the function of eight variables is just one function of three variables, times one function of the other five and so forth. In addition to that, to empower their deployed machine learning program, one technique that they recognized as useful was the <b>smoothing</b> of the functions they acquire before moving forward. By that I mean, they further approximate the mystery functions in an attempt to capture the important patterns, while leaving out noise or other phenomena. Like a value of 1.9999998 becomes a 2 and so on, auto-correcting along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: How do they combine these techniques and observations to solve the problem better?\n",
    "\n",
    "It turns out that the decomposition techniques they thought of proved to be quite successful. In truth, one of the reasons symbolic regression is a difficult task, rests in the curse of dimensionality. By that I mean that problems become exponentially worse and difficult to find as the number of variables increases.\n",
    "\n",
    "Nevertheless, in a step wise approach the \"Ai Feynman\" algorithm performs a number of methods on the datatables that prove to be rather successful in tackling the issues at hand. These are:\n",
    "\n",
    "\n",
    "![title](img/methods.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensional analysis happens at the start of the procedure. Whereby, the solution tests if the datatables have any of the simplifying properties we discussed. \n",
    "\n",
    "At this stage, the algorithm using a brute force approach that leverags a unit-table produced by the researchers, attempts to simplify before moving forward in the process. And, this proves quite useful at later stages. For example, if the solution using neural networks discovered that the only way it depends on column one and four is by a ratio of the two, then immediately the program can replace column one and four by one column that is their ratio and so forth.\n",
    "\n",
    "\n",
    "![title](img/dimensional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simiarly, once that is done, the program checks for symmetries and separabilities using the following algorithms by accessing the datatables and attempting to find an adequate formulation. We bare in mind that the RMSE is the new complexity criterion established at the beginning of this article.\n",
    "\n",
    "![title](img/symmetry.png)\n",
    "\n",
    "![title](img/sym.png)\n",
    "\n",
    "\n",
    "![title](img/separability.png)\n",
    "\n",
    "![title](img/sep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Results\n",
    "\n",
    "One can see by now that the key innovation lies in the combination of traditional fitting techniques with a neural network based strategy. This is in a way is an example of lossy data compression because once discussed properties are discovered and problems recursively reduced, the solution eliminates dependent variables and produces great results.\n",
    "\n",
    "On the same page, besides calculating the accuracy of each equation, the researchers also kept track of the complexity of the equations the neural network discovered. Hoping to see what \"Ai Feynman\" comes up with, they sought to strike a balance between accuracy and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the process of tackling the particular mystery of figuring out the kinetic energy formula and special relativity,\n",
    "here is what came out of the inner workings of their neural network:\n",
    "\n",
    "![title](img/inner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, one can observe that scientists tend to value formulas that do well on both complexity and accuracy. And, even though \"Ai Feynman\" was neither taught high school physics nor that a particular approximation is useful, it, all on its own found that <b> mass times velocity squared over two</b> is a really useful approximation for kinetic energy just from looking at the data.\n",
    "\n",
    "Not only is this solution useful for discovering exact laws, it is also great for finding useful approximations accross equations.\n",
    "\n",
    "![title](img/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the results, where the last row of the first table represents the performance of \"Ai Feynman 2.0\"\n",
    "As you can see they are able to discover a vast majority of the equations and they are doing significantly better than the previous state of the art algorithms.\n",
    "\n",
    "On the second table, you can also see how well they increased the robustness to noise of their solution. These numbers represent the percentage wise noise to that was added to the data. As you can see even if the noise was between 1% and 10%, the program is able to discover the equation for most of the datatables. This means that the robustness of the second version of their solution that leverages graph modularity is increased by 4 orders of magnitude compared to the previous version of the algorithm.\n",
    "\n",
    "https://ai-feynman.readthedocs.io/en/latest/index.html\n",
    "\n",
    "### Part 5: The question remains how can we use this solution?\n",
    "\n",
    "All you have to do to start using this algorithm and deploy it in your research is to run: <b>pip install feynman</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a look at how this is done, I provide here a Google Collab notebook containing all of the code you need with examples of how to use it.\n",
    "\n",
    "https://colab.research.google.com/drive/1BzjnVqle07Wt75AOwKd2G1HImmA7Kz6R?usp=sharing\n",
    "\n",
    "\n",
    "![title](img/use.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reading, I refer you to the following papers:\n",
    "\n",
    "Their initial attempt can be found here https://advances.sciencemag.org/content/advances/6/16/eaay2631.full.pdf\n",
    "\n",
    "Their second more improved version can be found here as well https://arxiv.org/pdf/2006.10782.pdf\n",
    "\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}